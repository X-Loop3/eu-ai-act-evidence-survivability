# Baseline

This directory defines the **baseline assumption** used in the EU AI Act Evidence Survivability Benchmark.

It exists to make one thing explicit:

> The benchmark does **not** test “compliance”.
> It tests whether an evidence bundle can survive **deadline pressure** without collapsing.

---

## What “baseline” means here

The baseline represents a **minimal, engineering-grade documentation scaffold** that a team can start from.

It assumes:
- No regulator approval
- No certification
- No operational controls
- No legal interpretation

Only structure.

---

## Relationship to the EU AI Act Layer (Lite)

The benchmark baseline is intentionally aligned with the **EU AI Act Layer (Lite)** published separately.

The Lite layer provides:
- A structured documentation map
- Stable terminology
- Explicit placeholders instead of silent gaps

This benchmark uses the same assumptions, but **does not require** adoption of the Lite layer.

Teams may:
- Use their own internal structure
- Use no structure at all
- Or use the Lite layer as a starting point

The benchmark result will reflect that choice.

---

## Why this matters

Most audit failures do not happen because teams lack documents.

They happen because:
- links break
- versions drift
- terminology diverges
- narratives contradict under stress

A baseline makes these failure modes **visible**.

---

## What this baseline is not

- Not a compliance framework
- Not a regulator-approved template
- Not legal advice
- Not an operational system

It is a **reference point** for stress-testing documentation coherence.

---

## If you want a ready-made baseline

If you want a free, copy-ready baseline aligned to this benchmark, see:

https://github.com/X-Loop3/eu-ai-act-layer-lite

Use is optional.
The benchmark remains valid without it.

---

## Summary

The baseline exists to answer a simple question:

> “What breaks first when time runs out?”

This directory makes that question explicit.
